{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unzip the exported dataset from CVAT  \n",
    "set zipfile name as CVAT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Setting\n",
    "\n",
    "zipfile_name = '1stdataset20230420'\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "with zipfile.ZipFile(f'{zipfile_name}.zip', 'r') as zf:\n",
    "    if not os.path.exists(f'{zipfile_name}'):\n",
    "        os.makedirs(f'{zipfile_name}')\n",
    "\n",
    "    zf.extractall(f'{zipfile_name}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Box Extraction\n",
    "`def image_names_parser(path)` -> return all image names from the annotations\n",
    "`def labels_parser(path, image_name)` -> return label boxs of corresponding image\n",
    "\n",
    "test: check if the box is in the right place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_names_parser(path):\n",
    "    tree = ET.parse(f'{path}/annotations.xml')\n",
    "    root = tree.getroot()\n",
    "    image_names = []\n",
    "    for image in root.findall('image'):\n",
    "        image_names.append(image.get('name'))\n",
    "    return image_names\n",
    "\n",
    "def labels_parser(path, image_name):\n",
    "    tree = ET.parse(f'{path}/annotations.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    labels = {}\n",
    "    for i, label in enumerate(root.iter('label')):\n",
    "        labels[label.find('name').text] = i\n",
    "\n",
    "    image_node = None\n",
    "    for image in root.findall('image'):\n",
    "        if image.get('name') == image_name:\n",
    "            image_node = image\n",
    "\n",
    "    boxs = []\n",
    "    coordinates = ['ytl', 'xtl', 'ybr', 'xbr']\n",
    "    for i in range(len(labels)):\n",
    "        boxs.append([])\n",
    "    for box in image_node.findall('box'):\n",
    "        points = []\n",
    "        for coordinate in coordinates:\n",
    "            points.append(int(box.get(coordinate).split(\".\")[0]))\n",
    "        boxs[labels[box.get('label')]].append(points)\n",
    "\n",
    "    return boxs\n",
    "boxs = labels_parser(zipfile_name, 'WLT_350_210926 P_2111131 WLT_350_210926__002 P_2111131_processed.JPG')\n",
    "for i in range(len(boxs)):\n",
    "    print(boxs[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the annotations is shown as they are on CVAT\n",
    "set the index of images and the feature type in the CVAT annotations\n",
    "\n",
    "Flag of the feature type\n",
    "- small strong hyperbola: 0\n",
    "- small weak hyperbola: 1\n",
    "- noise: 2\n",
    "- large strong hyperbola: 3\n",
    "- large weak hyberbola: 4\n",
    "- no feature: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Setting\n",
    "\n",
    "index = 0\n",
    "flag = 0\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "test_image = image_names_parser(zipfile_name)[index]\n",
    "boxs = labels_parser(zipfile_name, test_image)\n",
    "\n",
    "print(test_image)\n",
    "img = cv2.imread(f'processed/{test_image}')\n",
    "color = np.array([3, 67, 223])\n",
    "box_width = 3\n",
    "for box in boxs[flag]:\n",
    "    img[box[0] - box_width:box[0] + box_width, box[1]:box[3], :] = np.ones_like(img[box[0] - box_width:box[0] + box_width, box[1]:box[3], :]) * color\n",
    "    img[box[2] - box_width:box[2] + box_width, box[1]:box[3], :] = np.ones_like(img[box[2] - box_width:box[2] + box_width, box[1]:box[3], :]) * color\n",
    "    img[box[0]:box[2], box[1] - box_width:box[1] + box_width, :] = np.ones_like(img[box[0]:box[2], box[1] - box_width:box[1] + box_width, :]) * color\n",
    "    img[box[0]:box[2], box[3] - box_width:box[3] + box_width, :] = np.ones_like(img[box[0]:box[2], box[3] - box_width:box[3] + box_width, :]) * color\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_out_of_box(sliding_window, box):\n",
    "    if sliding_window[2] < box[0] or sliding_window[0] > box[2]:\n",
    "        return True\n",
    "    if sliding_window[3] < box[1] or sliding_window[1] > box[3]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_box(sliding_window, box, overlapping):\n",
    "    if check_out_of_box(sliding_window, box):\n",
    "        return False\n",
    "    \n",
    "    box_area = (box[2] - box[0]) * (box[3] - box[1])\n",
    "    overlap_area = (min(sliding_window[2], box[2]) - max(sliding_window[0], box[0])) * (min(sliding_window[3], box[3]) - max(sliding_window[1], box[1]))\n",
    "    if overlap_area / box_area < overlapping:\n",
    "        return False    \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(save_path, zipfile_name, image_index, image_name, boxs, overlapping, sliding_window_size=200, step=40):\n",
    "    img = cv2.imread(f'{save_path}/{image_name}')\n",
    "    feature_types = ['small strong hyperbola', 'small weak hyperbola', 'noise', 'large strong hyperbola', 'large weak hyberbola', 'no feature']\n",
    "    # exclude the left axis and data above 0 ms\n",
    "    # Boundary constant of images\n",
    "    IMAGE_START = 64\n",
    "    AXIS_ZERO = 121\n",
    "    image_shape = img.shape\n",
    "    image_start = [AXIS_ZERO, IMAGE_START]\n",
    "    print('image shape:', image_shape)\n",
    "    height = math.ceil((image_shape[0] - sliding_window_size - image_start[0]) / step)\n",
    "    width = math.ceil((image_shape[1] - sliding_window_size - image_start[1]) / step)\n",
    "\n",
    "    no_feature_index = len(feature_types) - 1\n",
    "    cropped_image_type = np.ones((height, width)) * no_feature_index\n",
    "    for k, box_list in enumerate(boxs):\n",
    "        for box in box_list:\n",
    "            box_index = [(b - image_start[0]) / step for b in box]\n",
    "            start = [math.floor(box_index[0]), math.floor(box_index[1])]\n",
    "            end = [math.ceil(box_index[2]), math.ceil(box_index[3])]\n",
    "            sub_height_start = int(math.floor((AXIS_ZERO - image_start[0]) / step))\n",
    "            sub_width_start = int(math.floor((IMAGE_START - image_start[1]) / step))\n",
    "            for j in range(max(start[0] - int(sliding_window_size/step), sub_height_start), min(end[0] + int(sliding_window_size/step), height)):\n",
    "                for i in range(max(start[1] - int(sliding_window_size/step), sub_width_start), min(end[1] + int(sliding_window_size/step), width)):\n",
    "                    if cropped_image_type[j, i] == no_feature_index and check_box([j, i, j + int(sliding_window_size/step), i + int(sliding_window_size/step)], box_index, overlapping):\n",
    "                        cropped_image_type[j, i] = k\n",
    "    \n",
    "    # create folders for cropped images\n",
    "    for i in range(len(feature_types)):\n",
    "        if not os.path.exists(f'{save_path}/cropped/{sliding_window_size}_{step}/{zipfile_name}/{image_index}/{i}'):\n",
    "            os.makedirs(f'{save_path}/cropped/{sliding_window_size}_{step}/{zipfile_name}/{image_index}/{i}')\n",
    "    # output cropped images\n",
    "    for j in range(height):\n",
    "        count = [0] * len(feature_types)\n",
    "        for k in range(len(count)):\n",
    "            for path in os.listdir(f'{save_path}/cropped/{sliding_window_size}_{step}/{zipfile_name}/{image_index}/{k}'):\n",
    "                if path.startswith(f'{j * step}_'):\n",
    "                    image_num = int(path.split(\".jpg\")[0].split(\"_\")[1])\n",
    "                    count[k] = max(count[k], image_num)\n",
    "\n",
    "        for i in range(width):\n",
    "            sliding_window = [AXIS_ZERO + j * step, IMAGE_START + i * step, AXIS_ZERO + sliding_window_size + j * step, IMAGE_START + sliding_window_size + i * step]\n",
    "            sub_img = img[sliding_window[0]:sliding_window[2], sliding_window[1]:sliding_window[3]]\n",
    "            feature_type = int(cropped_image_type[j, i])\n",
    "            cv2.imwrite(f'{save_path}/cropped/{sliding_window_size}_{step}/{zipfile_name}/{image_index}/{feature_type}/{j * step}_{count[feature_type]}.jpg', sub_img)\n",
    "            count[feature_type] += 1\n",
    "\n",
    "    print(f'Finished generating images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Setting\n",
    "\n",
    "IMAGE_START = 64\n",
    "AXIS_ZERO = 121\n",
    "\n",
    "img = cv2.imread(f'processed/{test_image}')\n",
    "box_width = 3\n",
    "image_shape = img.shape\n",
    "image_start = [AXIS_ZERO, IMAGE_START]\n",
    "sliding_window_size = 200\n",
    "step = 40\n",
    "\n",
    "height = math.ceil((image_shape[0] - sliding_window_size - image_start[0]) / step)\n",
    "width = math.ceil((image_shape[1] - sliding_window_size - image_start[1]) / step)\n",
    "\n",
    "j, i = 0, 0\n",
    "sliding_window = [AXIS_ZERO + j * step, IMAGE_START + i * step, AXIS_ZERO + sliding_window_size + j * step, IMAGE_START + sliding_window_size + i * step]\n",
    "sub_img = img[sliding_window[0]:sliding_window[2], sliding_window[1]:sliding_window[3]]\n",
    "            \n",
    "plt.imshow(sub_img)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the save path of cropped images\n",
    "The cropped image save in the processed directory in default.\n",
    "\n",
    "### Parameter\n",
    "- save_path: default in 'processed'\n",
    "- flag: feature type. If all kind of feature is required, run through flag 0-5\n",
    "- sliding_window_size: size of the cropped images. 200 x 200 pixels in default\n",
    "- step: step of sliding window. 100 pixels in default\n",
    "- overlapping: acceptance rate of overlapping of sliding windows and feature box. 0.8 in default, which means that sliding window covers above 80% of feature box\n",
    "\n",
    "Flag of the feature type\n",
    "- small strong hyperbola: 0\n",
    "- small weak hyperbola: 1\n",
    "- noise: 2\n",
    "- large strong hyperbola: 3\n",
    "- large weak hyberbola: 4\n",
    "- no feature: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Setting\n",
    "\n",
    "save_path = 'unprocessed_images'\n",
    "sliding_window_size = 200\n",
    "step = 40\n",
    "overlapping = 0.8\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "image_names = image_names_parser(zipfile_name)\n",
    "for image_index, image_name in enumerate(image_names):\n",
    "    boxs = labels_parser(zipfile_name, image_name)\n",
    "    image_name = image_name.replace('_processed', '')\n",
    "    print(f'cropping {image_name} ...')\n",
    "    crop_image(save_path, zipfile_name, image_index, image_name, boxs, overlapping, sliding_window_size=sliding_window_size, step=step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
